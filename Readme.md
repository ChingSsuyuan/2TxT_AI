# 📸📝 图像标题生成模型 (Image Captioning using CNN + Transformer)

## 📊 1. 数据表示与处理

### 🎨 图像表示
- **输入形式：** RGB彩色图像
- **尺寸标准化：** 通常调整为 `224×224` 或 `256×256` 像素
- **维度：** 每个样本形状为 `[3, H, W]`
    - 3 个通道 (R, G, B)
    - H = 高度
    - W = 宽度

#### ✅ 预处理：
- 调整大小
- 标准化（减去均值除以标准差）
- 数据增强（随机裁剪、翻转等）

---

### 📝 文本表示
- **输入形式：** 文本标题字符串
- **分词：** 将标题分解为单词或子词单元
- **维度：** 每个样本形状为 `[L]`
    - L = 标题长度（词/子词数量）

#### ✅ 预处理：
- 构建词汇表
- 转换为索引序列
- 添加特殊标记（`<START>`，`<END>`）
- 填充批量样本到相同长度

---

## 🧠 2. 模型架构分解

### 🖼️ 图像编码部分 (卷积神经网络)
- **位置：** 模型的第一部分，处理视觉输入
- **实现：** 预训练 CNN (如 ResNet, ViT)
- **输入：** `[批量大小, 3, H, W]` 图像张量
- **输出：**
    - 全局图像特征 `[批量大小, 特征维度]`
    - 或特征图 `[批量大小, 特征维度, h, w]`
- **作用：** 提取图像的语义特征和内容表示

---

### 🔗 连接层 (注意力机制)
- **位置：** 图像编码器和文本解码器之间
- **实现：**
    - 自注意力：使图像特征关注自身的重要区域
    - 交叉注意力：连接图像特征和文本表示
- **输入：**
    - 图像特征 `[批量大小, 特征维度]`
    - 当前生成的文本嵌入 `[批量大小, 当前长度, 嵌入维度]`
- **输出：** 上下文向量 `[批量大小, 当前长度, 嵌入维度]`
- **作用：** 使解码器能够集中于图像的相关部分

---

### ✨ 文本解码部分 (Transformer模型)
- **位置：** 模型的后半部分，生成文本
- **实现：** Transformer 解码器层叠
- **输入：**
    - 编码的图像特征
    - 已生成的部分序列
- **输出：** 下一个词的预测概率分布
- **作用：** 顺序生成描述图像的文本标题

---

## 📈 3. 详细流程图解

### 🚀 训练流程
1. **图像编码：**
    - 图像进入 CNN
    - CNN 提取特征表示
    - 特征可能经过一个线性映射调整维度
2. **文本表示：**
    - 标题被分词并转换为索引
    - 索引通过嵌入层转换为向量
3. **解码过程：**
    - 输入 `<START>` 标记
    - 解码器接收图像特征和当前文本
    - 注意力机制计算每个位置的上下文
    - Transformer 层处理上下文信息
    - 线性层和 softmax 预测下一个词
4. **损失计算：**
    - 使用交叉熵损失比较预测和实际词
    - 反向传播更新参数

---

### 🤖 推理流程
1. **图像编码：** 与训练相同
2. **自回归解码：**
    - 从 `<START>` 标记开始
    - 循环执行解码步骤：
        - 生成下一个词的概率
        - 选择最可能的词或使用 beam search
        - 将选择的词添加到序列
    - 继续直到生成 `<END>` 或达到最大长度

---

## 🎯 4. 各组件具体作用

### 🖼️ 卷积神经网络作用
- 提取层次化的视觉特征
- 识别图像中的对象、场景和视觉属性
- 将像素级信息压缩为语义表示

### 🔍 注意力机制作用
- 让模型专注于与当前生成词相关的图像区域
- 动态调整不同图像部分的重要性
- 在视觉和语言模态间建立对应关系

### 🧠 Transformer模型作用
- 处理序列数据，生成连贯文本
- 通过自注意力机制捕捉长距离依赖
- 使用多头注意力获取不同的表示视角

---

## 📚 5. 数据流动维度变化示例

假设：

- 批量大小 = 32
- 图像大小 = 224×224
- 最大标题长度 = 20
- 特征维度 = 2048
- 嵌入维度 = 512

---

### 📏 具体维度变化：
1. **图像输入：** `[32, 3, 224, 224]`
2. **CNN 输出：** `[32, 2048]`
3. **文本输入索引：** `[32, 20]`
4. **文本嵌入：** `[32, 20, 512]`
5. **注意力层输出：** `[32, 20, 512]`
6. **解码器输出：** `[32, 20, 词汇表大小]`